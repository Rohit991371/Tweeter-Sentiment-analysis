{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...      -1.0\n",
       "1       talk all the nonsense and continue all the dra...       0.0\n",
       "2       what did just say vote for modi  welcome bjp t...       1.0\n",
       "3       asking his supporters prefix chowkidar their n...       1.0\n",
       "4       answer who among these the most powerful world...       1.0\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
       "162977  did you cover her interaction forum where she ...       0.0\n",
       "162978  there big project came into india modi dream p...       0.0\n",
       "162979  have you ever listen about like gurukul where ...       1.0\n",
       "\n",
       "[162980 rows x 2 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original = pd.read_csv('Twitter_Data.csv')\n",
    "data_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    4\n",
       "category      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...      -1.0\n",
       "1       talk all the nonsense and continue all the dra...       0.0\n",
       "2       what did just say vote for modi  welcome bjp t...       1.0\n",
       "3       asking his supporters prefix chowkidar their n...       1.0\n",
       "4       answer who among these the most powerful world...       1.0\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
       "162977  did you cover her interaction forum where she ...       0.0\n",
       "162978  there big project came into india modi dream p...       0.0\n",
       "162979  have you ever listen about like gurukul where ...       1.0\n",
       "\n",
       "[162980 rows x 2 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    4\n",
       "category      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where either clean_text or category has NaN values\n",
    "data = data.dropna(subset=['clean_text', 'category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    0\n",
       "category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'category' column\n",
    "print(data_cleaned['category'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\886648224.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['category'].replace({1: 'positive', 0: 'neutral', -1: 'negative'}, inplace=True)\n",
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\886648224.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['category'].replace({1: 'positive', 0: 'neutral', -1: 'negative'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# data['category'].replace(1, 'positive', inplace=True)\n",
    "data['category'].replace({1: 'positive', 0: 'neutral', -1: 'negative'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         when modi promised “minimum government maximum...\n",
       "1         talk all the nonsense and continue all the dra...\n",
       "2         what did just say vote for modi  welcome bjp t...\n",
       "3         asking his supporters prefix chowkidar their n...\n",
       "4         answer who among these the most powerful world...\n",
       "                                ...                        \n",
       "162975    why these 456 crores paid neerav modi not reco...\n",
       "162976    dear rss terrorist payal gawar what about modi...\n",
       "162977    did you cover her interaction forum where she ...\n",
       "162978    there big project came into india modi dream p...\n",
       "162979    have you ever listen about like gurukul where ...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EMOTICONS_EMO dictionary from the file\n",
    "import sys\n",
    "sys.path.insert(0, 'D:/DSM/Assignment/3.Project_Twitter Sentimental Analysis/')\n",
    "from emo_unicode import EMOTICONS_EMO\n",
    "from emo_unicode import EMOJI_UNICODE\n",
    "from emo_unicode import __all__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Happy face or smiley How are you? Frown, sad, angry or pouting\n"
     ]
    }
   ],
   "source": [
    "def replace_emoticons(text):\n",
    "    # Replace emoticons in the text with their meanings\n",
    "    for emoticon, meaning in EMOTICONS_EMO.items():\n",
    "        # Use re.escape to escape special characters in emoticons\n",
    "        text = re.sub(re.escape(emoticon), meaning, text)\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello :) How are you? :‑(\"\n",
    "clean_text = replace_emoticons(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sad Frown, sad, angry or pouting)'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am sad :()\"\n",
    "replace_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning :sparkling_heart:! Let's go for a walk :person_walking:‍:male_sign:️.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from emo_unicode import EMOJI_UNICODE  \n",
    "\n",
    "def replace_emojis(text):\n",
    "    # Replace emojis in the text with their meanings\n",
    "    for meaning, emoji in EMOJI_UNICODE.items():\n",
    "        # Use re.escape to escape special characters in emojis\n",
    "        text = re.sub(re.escape(emoji), meaning, text)\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text_with_emojis = \"Good morning 💖! Let's go for a walk 🚶‍♂️.\"\n",
    "clean_text = replace_emojis(text_with_emojis)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2832120601.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(replace_emoticons)\n"
     ]
    }
   ],
   "source": [
    "# data['clean_text'] = data['clean_text'].apply(replace_emoticons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons = \"emoticons_removed.csv\"\n",
    "# data.to_csv(emoticons, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\3743668206.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(replace_emojis)\n"
     ]
    }
   ],
   "source": [
    "# data['clean_text'] = data['clean_text'].apply(replace_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji = \"emoji_removed.csv\"\n",
    "# data.to_csv(emoji, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\3925427806.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...  negative\n",
       "1       talk all the nonsense and continue all the dra...   neutral\n",
       "2       what did just say vote for modi  welcome bjp t...  positive\n",
       "3       asking his supporters prefix chowkidar their n...  positive\n",
       "4       answer who among these the most powerful world...  positive\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...  negative\n",
       "162976  dear rss terrorist payal gawar what about modi...  negative\n",
       "162977  did you cover her interaction forum where she ...   neutral\n",
       "162978  there big project came into india modi dream p...   neutral\n",
       "162979  have you ever listen about like gurukul where ...  positive\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\3495459935.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['clean_text'].fillna('', inplace=True)\n",
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\3495459935.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    # Compiling a regex pattern to match HTML tags\n",
    "    pattern = re.compile(r'<.*?>')\n",
    "    # Removing the matched HTML tags from the input text\n",
    "    return pattern.sub('', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\356361529.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(remove_tags)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\3875691980.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(remove_url)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...  negative\n",
       "1       talk all the nonsense and continue all the dra...   neutral\n",
       "2       what did just say vote for modi  welcome bjp t...  positive\n",
       "3       asking his supporters prefix chowkidar their n...  positive\n",
       "4       answer who among these the most powerful world...  positive\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...  negative\n",
       "162976  dear rss terrorist payal gawar what about modi...  negative\n",
       "162977  did you cover her interaction forum where she ...   neutral\n",
       "162978  there big project came into india modi dream p...   neutral\n",
       "162979  have you ever listen about like gurukul where ...  positive\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Removing Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude = string.punctuation\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\859749392.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(remove_punc)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Chat Words Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher'}\n"
     ]
    }
   ],
   "source": [
    "chat_words = {}\n",
    "\n",
    "with open('slang.txt', mode='r') as myfile:\n",
    "    for line in myfile:\n",
    "        if '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            chat_words[key.strip()] = value.strip()\n",
    "\n",
    "# Check the dictionary to verify\n",
    "print(chat_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'ILU: I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laugh My A.. Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The A..',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F...',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait...',\n",
       " '7K': 'Sick:-D Laugher'}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_words_treatment(text):\n",
    "    new_text = []\n",
    "    words = text.split()\n",
    "    \n",
    "    for w in words:\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    \n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2602634632.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(chat_words_treatment)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(chat_words_treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Spelling Corrections.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install TextBlob\n",
    "# from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://norvig.com/spell-correct.html\n",
    "\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "# def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# # WORDS = Counter(words(open('big.txt').read()))\n",
    "# WORDS = data['clean_text']\n",
    "\n",
    "# def P(word, N=sum(WORDS.values())): \n",
    "#     \"Probability of `word`.\"\n",
    "#     return WORDS[word] / N\n",
    "\n",
    "# def correction(word): \n",
    "#     \"Most probable spelling correction for word.\"\n",
    "#     return max(candidates(word), key=P)\n",
    "\n",
    "# def candidates(word): \n",
    "#     \"Generate possible spelling corrections for word.\"\n",
    "#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "# def known(words): \n",
    "#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "#     return set(w for w in words if w in WORDS)\n",
    "\n",
    "# def edits1(word):\n",
    "#     \"All edits that are one edit away from `word`.\"\n",
    "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# def edits2(word): \n",
    "#     \"All edits that are two edits away from `word`.\"\n",
    "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This is a smple exmple of misspeled wrds\n",
      "Corrected: this his at simple example off misspelled words\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Create a dictionary of word frequencies\n",
    "WORDS = Counter(words(' '.join(data['clean_text'])))\n",
    "\n",
    "def get_candidates(word):\n",
    "    \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
    "    # Original word\n",
    "    yield word\n",
    "    \n",
    "    # One-edit distance words\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Deletions\n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            yield L + R[1:]\n",
    "    \n",
    "    # Replacements\n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            for c in letters:\n",
    "                yield L + c + R[1:]\n",
    "    \n",
    "    # Insertions\n",
    "    for L, R in splits:\n",
    "        for c in letters:\n",
    "            yield L + c + R\n",
    "\n",
    "def correct_spelling(word):\n",
    "    \"\"\"Suggest the most probable spelling correction for word.\"\"\"\n",
    "    candidates = get_candidates(word)\n",
    "    return max((c for c in candidates if c in WORDS), key=WORDS.get, default=word)\n",
    "\n",
    "def spell_check_text(text):\n",
    "    \"\"\"Check spelling of all words in the given text.\"\"\"\n",
    "    words = text.split()\n",
    "    corrected = [correct_spelling(word) for word in words]\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "# Example usage\n",
    "misspelled_text = \"This is a smple exmple of misspeled wrds\"\n",
    "corrected_text = spell_check_text(misspelled_text)\n",
    "print(f\"Original: {misspelled_text}\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spell'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_text('spel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2162592157.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(spell_check_text)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['clean_text'] = data['clean_text'].apply(spell_check_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         when modi promised minimum government maximum ...\n",
       "1         talk all the nonsense and continue all the dra...\n",
       "2         that did just says vote for modi welcome bjp o...\n",
       "3         asking this supporters prefix chowkidar their ...\n",
       "4         answer who among there the must powerful would...\n",
       "                                ...                        \n",
       "162975    who there 456 crore said neerav modi not recov...\n",
       "162976    year rss terrorists pagal pawar that about mod...\n",
       "162977     did you over here interaction form there the let\n",
       "162978    there big project same into india modi dream p...\n",
       "162979    have you even listen about like gurukul there ...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a new DataFrame\n",
    "spelling_corrected = pd.DataFrame({'corrected': data['clean_text']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         when modi promised minimum government maximum ...\n",
       "1         talk all the nonsense and continue all the dra...\n",
       "2         that did just says vote for modi welcome bjp o...\n",
       "3         asking this supporters prefix chowkidar their ...\n",
       "4         answer who among there the must powerful would...\n",
       "                                ...                        \n",
       "162975    who there 456 crore said neerav modi not recov...\n",
       "162976    year rss terrorists pagal pawar that about mod...\n",
       "162977     did you over here interaction form there the let\n",
       "162978    there big project same into india modi dream p...\n",
       "162979    have you even listen about like gurukul there ...\n",
       "Name: corrected, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_corrected['corrected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_corrected.to_csv('spelling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2820148359.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(remove_stopwords)\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          modi promised minimum government maximum gove...\n",
       "1             talk   nonsense  continue   drama  vote  modi\n",
       "2            says vote  modi welcome bjp old  rahul  man...\n",
       "3         asking  supporters prefix chowkidar  name modi...\n",
       "4         answer  among   must powerful would leader tod...\n",
       "                                ...                        \n",
       "162975      456 crore said neerav modi  recovered  congr...\n",
       "162976    year rss terrorists pagal pawar   modi killing...\n",
       "162977                               interaction form   let\n",
       "162978     big project   india modi dream project   happ...\n",
       "162979      even listen  like gurukul  discipline  maint...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          modi promised minimum government maximum gove...\n",
       "1             talk   nonsense  continue   drama  vote  modi\n",
       "2            says vote  modi welcome bjp old  rahul  man...\n",
       "3         asking  supporters prefix chowkidar  name modi...\n",
       "4         answer  among   must powerful would leader tod...\n",
       "                                ...                        \n",
       "162975      456 crore said neerav modi  recovered  congr...\n",
       "162976    year rss terrorists pagal pawar   modi killing...\n",
       "162977                               interaction form   let\n",
       "162978     big project   india modi dream project   happ...\n",
       "162979      even listen  like gurukul  discipline  maint...\n",
       "Name: stopword, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new DataFrame\n",
    "stopwords_removed = pd.DataFrame({'stopword': data['clean_text']})\n",
    "stopwords_removed['stopword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed.to_csv('stopwords_removed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          modi promised minimum government maximum gove...\n",
       "1             talk   nonsense  continue   drama  vote  modi\n",
       "2            says vote  modi welcome bjp old  rahul  man...\n",
       "3         asking  supporters prefix chowkidar  name modi...\n",
       "4         answer  among   must powerful would leader tod...\n",
       "                                ...                        \n",
       "162975      456 crore said neerav modi  recovered  congr...\n",
       "162976    year rss terrorists pagal pawar   modi killing...\n",
       "162977                               interaction form   let\n",
       "162978     big project   india modi dream project   happ...\n",
       "162979      even listen  like gurukul  discipline  maint...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-download punkt\n",
    "# nltk.download('punkt', download_dir=r'C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-download punkt\n",
    "# nltk.download('punkt', download_dir=r'D:\\DSM\\Assignment\\3.Project_Twitter Sentimental Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = Tokenizer(lower = False)\n",
    "# data['clean_text'] = token.texts_to_sequences(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text,language=\"english\"):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\4288043612.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['clean_text'] = data['clean_text'].apply(sent_tokenize)\n"
     ]
    }
   ],
   "source": [
    "# data['clean_text'] = data['clean_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [modi, promised, minimum, government, maximum,...\n",
       "1             [talk, nonsense, continue, drama, vote, modi]\n",
       "2         [says, vote, modi, welcome, bjp, old, rahul, m...\n",
       "3         [asking, supporters, prefix, chowkidar, name, ...\n",
       "4         [answer, among, must, powerful, would, leader,...\n",
       "                                ...                        \n",
       "162975    [456, crore, said, neerav, modi, recovered, co...\n",
       "162976    [year, rss, terrorists, pagal, pawar, modi, ki...\n",
       "162977                             [interaction, form, let]\n",
       "162978    [big, project, india, modi, dream, project, ha...\n",
       "162979    [even, listen, like, gurukul, discipline, main...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized = \"tokenized_text.csv\"\n",
    "# data.to_csv(tokenized, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [modi, promised, minimum, government, maximum,...\n",
       "1             [talk, nonsense, continue, drama, vote, modi]\n",
       "2         [says, vote, modi, welcome, bjp, old, rahul, m...\n",
       "3         [asking, supporters, prefix, chowkidar, name, ...\n",
       "4         [answer, among, must, powerful, would, leader,...\n",
       "                                ...                        \n",
       "162975    [456, crore, said, neerav, modi, recovered, co...\n",
       "162976    [year, rss, terrorists, pagal, pawar, modi, ki...\n",
       "162977                             [interaction, form, let]\n",
       "162978    [big, project, india, modi, dream, project, ha...\n",
       "162979    [even, listen, like, gurukul, discipline, main...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['modi', 'promised', 'minimum', 'government', ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['talk', 'nonsense', 'continue', 'drama', 'vot...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['asking', 'supporters', 'prefix', 'chowkidar'...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['answer', 'among', 'must', 'powerful', 'would...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162964</th>\n",
       "      <td>['456', 'crore', 'said', 'neerav', 'modi', 're...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162965</th>\n",
       "      <td>['year', 'rss', 'terrorists', 'pagal', 'pawar'...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162966</th>\n",
       "      <td>['interaction', 'form', 'let']</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162967</th>\n",
       "      <td>['big', 'project', 'india', 'modi', 'dream', '...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162968</th>\n",
       "      <td>['even', 'listen', 'like', 'gurukul', 'discipl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       ['modi', 'promised', 'minimum', 'government', ...  negative\n",
       "1       ['talk', 'nonsense', 'continue', 'drama', 'vot...   neutral\n",
       "2       ['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...  positive\n",
       "3       ['asking', 'supporters', 'prefix', 'chowkidar'...  positive\n",
       "4       ['answer', 'among', 'must', 'powerful', 'would...  positive\n",
       "...                                                   ...       ...\n",
       "162964  ['456', 'crore', 'said', 'neerav', 'modi', 're...  negative\n",
       "162965  ['year', 'rss', 'terrorists', 'pagal', 'pawar'...  negative\n",
       "162966                     ['interaction', 'form', 'let']   neutral\n",
       "162967  ['big', 'project', 'india', 'modi', 'dream', '...   neutral\n",
       "162968  ['even', 'listen', 'like', 'gurukul', 'discipl...  positive\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = pd.read_csv('tokenized_text.csv')\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['modi', 'promised', 'minimum', 'government', ...\n",
       "1         ['talk', 'nonsense', 'continue', 'drama', 'vot...\n",
       "2         ['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...\n",
       "3         ['asking', 'supporters', 'prefix', 'chowkidar'...\n",
       "4         ['answer', 'among', 'must', 'powerful', 'would...\n",
       "                                ...                        \n",
       "162964    ['456', 'crore', 'said', 'neerav', 'modi', 're...\n",
       "162965    ['year', 'rss', 'terrorists', 'pagal', 'pawar'...\n",
       "162966                       ['interaction', 'form', 'let']\n",
       "162967    ['big', 'project', 'india', 'modi', 'dream', '...\n",
       "162968    ['even', 'listen', 'like', 'gurukul', 'discipl...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['modi', 'promised', 'minimum', 'government', ...\n",
       "1         ['talk', 'nonsense', 'continue', 'drama', 'vot...\n",
       "2         ['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...\n",
       "3         ['asking', 'supporters', 'prefix', 'chowkidar'...\n",
       "4         ['answer', 'among', 'must', 'powerful', 'would...\n",
       "                                ...                        \n",
       "162964    ['456', 'crore', 'said', 'neerav', 'modi', 're...\n",
       "162965    ['year', 'rss', 'terrorists', 'pagal', 'pawar'...\n",
       "162966                       ['interaction', 'form', 'let']\n",
       "162967    ['big', 'project', 'india', 'modi', 'dream', '...\n",
       "162968    ['even', 'listen', 'like', 'gurukul', 'discipl...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed = \"stemmed_text.csv\"\n",
    "# data.to_csv(stemmed, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "data[\"clean_text\"] = data[\"clean_text\"].apply(lambda text: stem_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promis\n"
     ]
    }
   ],
   "source": [
    "print(stem_words('promised'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['modi', 'promised', 'minimum', 'government', ...\n",
       "1         ['talk', 'nonsense', 'continue', 'drama', 'vot...\n",
       "2         ['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...\n",
       "3         ['asking', 'supporters', 'prefix', 'chowkidar'...\n",
       "4         ['answer', 'among', 'must', 'powerful', 'would...\n",
       "                                ...                        \n",
       "162964    ['456', 'crore', 'said', 'neerav', 'modi', 're...\n",
       "162965    ['year', 'rss', 'terrorists', 'pagal', 'pawar'...\n",
       "162966                       ['interaction', 'form', 'let']\n",
       "162967    ['big', 'project', 'india', 'modi', 'dream', '...\n",
       "162968    ['even', 'listen', 'like', 'gurukul', 'discipl...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize('says'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['modi', 'promised', 'minimum', 'government', ...\n",
       "1         ['talk', 'nonsense', 'continue', 'drama', 'vot...\n",
       "2         ['says', 'vote', 'modi', 'welcome', 'bjp', 'ol...\n",
       "3         ['asking', 'supporters', 'prefix', 'chowkidar'...\n",
       "4         ['answer', 'among', 'must', 'powerful', 'would...\n",
       "                                ...                        \n",
       "162964    ['456', 'crore', 'said', 'neerav', 'modi', 're...\n",
       "162965    ['year', 'rss', 'terrorists', 'pagal', 'pawar'...\n",
       "162966                       ['interaction', 'form', 'let']\n",
       "162967    ['big', 'project', 'india', 'modi', 'dream', '...\n",
       "162968    ['even', 'listen', 'like', 'gurukul', 'discipl...\n",
       "Name: clean_text, Length: 162969, dtype: object"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2773966980.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['category'].replace({'positive': 2, 'neutral' : 0, 'negative' : 1}, inplace=True)\n",
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_25768\\2773966980.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['category'].replace({'positive': 2, 'neutral' : 0, 'negative' : 1}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data['category'].replace({'positive': 2, 'neutral' : 0, 'negative' : 1}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data['clean_text']\n",
    "y_data = data['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='category'>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.groupby('category').count().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shapes:\n",
      "X_train shape: (130375,)\n",
      "y_train shape: (130375,)\n",
      "\n",
      "Test Set Shapes:\n",
      "X_test shape: (32594,)\n",
      "y_test shape: (32594,)\n",
      "\n",
      "Sample X_train data:\n",
      "63424     ['media', 'guys', 'even', 'think', 'straight',...\n",
      "78454                                              ['name']\n",
      "70128     ['space', 'debris', 'problem', 'aware', 'new',...\n",
      "157068    ['today', 'interaction', 'modi', 'phenomenal',...\n",
      "120884    ['modi', 'bjp', 'want', 'change', 'niti', 'gad...\n",
      "Name: clean_text, dtype: object \n",
      "\n",
      "Sample X_test data:\n",
      "42228     ['new', 'flash', 'modi', 'address', 'nation', ...\n",
      "22034     ['according', 'congress', 'ecosystem', 'bjp', ...\n",
      "79981     ['friends', 'guys', 'get', 'drdo', 'best', 'mo...\n",
      "118492    ['modi', 'violate', 'model', 'come', 'conduct'...\n",
      "12814     ['rahul', 'gandhi', 'asked', 'modi', 'nirav', ...\n",
      "Name: clean_text, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print('Train Set Shapes:')\n",
    "print(f'X_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}\\n')\n",
    "\n",
    "print('Test Set Shapes:')\n",
    "print(f'X_test shape: {x_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}\\n')\n",
    "\n",
    "print('Sample X_train data:')\n",
    "print(x_train[:5], '\\n')  # Print the first 5 rows of x_train\n",
    "\n",
    "print('Sample X_test data:')\n",
    "print(x_test[:5], '\\n')  # Print the first 5 rows of x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_onehot = to_categorical(y_train, num_classes=3)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shapes:\n",
      "X_train shape: (130375,)\n",
      "y_train_onehot shape: (130375, 3)\n",
      "\n",
      "Test Set Shapes:\n",
      "X_test shape: (32594,)\n",
      "y_test_onehot shape: (32594, 3)\n",
      "\n",
      "Sample X_train data:\n",
      "63424     ['media', 'guys', 'even', 'think', 'straight',...\n",
      "78454                                              ['name']\n",
      "70128     ['space', 'debris', 'problem', 'aware', 'new',...\n",
      "157068    ['today', 'interaction', 'modi', 'phenomenal',...\n",
      "120884    ['modi', 'bjp', 'want', 'change', 'niti', 'gad...\n",
      "Name: clean_text, dtype: object \n",
      "\n",
      "Sample X_test data:\n",
      "42228     ['new', 'flash', 'modi', 'address', 'nation', ...\n",
      "22034     ['according', 'congress', 'ecosystem', 'bjp', ...\n",
      "79981     ['friends', 'guys', 'get', 'drdo', 'best', 'mo...\n",
      "118492    ['modi', 'violate', 'model', 'come', 'conduct'...\n",
      "12814     ['rahul', 'gandhi', 'asked', 'modi', 'nirav', ...\n",
      "Name: clean_text, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the shapes of the datasets\n",
    "print('Train Set Shapes:')\n",
    "print(f'X_train shape: {x_train.shape}')\n",
    "print(f'y_train_onehot shape: {y_train_onehot.shape}\\n')\n",
    "\n",
    "print('Test Set Shapes:')\n",
    "print(f'X_test shape: {x_test.shape}')\n",
    "print(f'y_test_onehot shape: {y_test_onehot.shape}\\n')\n",
    "\n",
    "print('Sample X_train data:')\n",
    "print(x_train[:5], '\\n')  # Print the first 5 rows of x_train\n",
    "\n",
    "print('Sample X_test data:')\n",
    "print(x_test[:5], '\\n')  # Print the first 5 rows of x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Set\n",
      "\n",
      "63424     2\n",
      "78454     0\n",
      "70128     2\n",
      "157068    2\n",
      "120884    2\n",
      "         ..\n",
      "119879    0\n",
      "103694    0\n",
      "131932    2\n",
      "146867    2\n",
      "121958    2\n",
      "Name: category, Length: 130375, dtype: int64 \n",
      "\n",
      "\n",
      "\n",
      "42228     0\n",
      "22034     2\n",
      "79981     1\n",
      "118492    1\n",
      "12814     0\n",
      "         ..\n",
      "47104     2\n",
      "33631     1\n",
      "93675     0\n",
      "37756     0\n",
      "132995    2\n",
      "Name: category, Length: 32594, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Y Set\\n')\n",
    "print(y_train,'\\n')\n",
    "print('\\n')\n",
    "print(y_test,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Set\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]] \n",
      "\n",
      "\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Y Set\\n')\n",
    "print(y_train_onehot,'\\n')\n",
    "print('\\n')\n",
    "print(y_test_onehot,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "        \n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to Use Each Method: <br><br>\n",
    "Use Bag of Words if:<br>\n",
    "You are working with traditional machine learning models (e.g., Logistic Regression, SVM).<br>\n",
    "You do not need to capture the word order.<br><br>\n",
    "Use pad_sequences if:<br>\n",
    "You are working with deep learning models like LSTM, GRU, CNN, or other sequential models, which require a fixed sequence length.<br>\n",
    "The order of words matters (e.g., for sentiment analysis, language modeling, or sequence prediction).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def encode_and_pad_tokens(x_train, x_test, max_words=10000, max_length=get_max_length()):\n",
    "    \"\"\"\n",
    "    Encode tokenized and preprocessed text data, create sequences, and pad them.\n",
    "    \n",
    "    Args:\n",
    "    x_train (list): List of tokenized training samples (list of lists)\n",
    "    x_test (list): List of tokenized test samples (list of lists)\n",
    "    max_words (int): Maximum number of words to keep in the vocabulary\n",
    "    max_length (int): Maximum length of sequences (default is 140 based on your data)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (padded_x_train, padded_x_test, tokenizer, total_words)\n",
    "    \"\"\"\n",
    "    #  out-of-vocabulary (OOV).\n",
    "    # Initialize and fit the Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    # Convert tokens to sequences\n",
    "    x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x_train = pad_sequences(x_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "    padded_x_test = pad_sequences(x_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Get total words\n",
    "    total_words = min(len(tokenizer.word_index) + 1, max_words)  # +1 for padding token\n",
    "    \n",
    "    \n",
    "    return padded_x_train, padded_x_test, tokenizer, total_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x_train, padded_x_test, tokenizer, total_words = encode_and_pad_tokens(x_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "int32\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# For binary classification, use int32 or float32\n",
    "# For multi-class classification, use int32 for sparse labels or float32 for one-hot encoded labels\n",
    "# For regression, use float32\n",
    "print(padded_x_train.dtype)\n",
    "print(padded_x_test.dtype)\n",
    "print(y_train_onehot.dtype)\n",
    "print(y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both x_train and x_test to float\n",
    "padded_x_train = padded_x_train.astype(np.float32)\n",
    "padded_x_test = padded_x_test.astype(np.float32)\n",
    "y_train_onehot = y_train_onehot.astype(np.float32)\n",
    "y_test_onehot = y_test_onehot.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(padded_x_train.dtype)\n",
    "print(padded_x_test.dtype)\n",
    "print(y_train_onehot.dtype)\n",
    "print(y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[7.700e+01 1.070e+02 2.400e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.140e+02 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [4.900e+01 1.479e+03 2.160e+02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.260e+02 2.270e+02 2.836e+03 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [5.840e+02 4.310e+02 4.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.110e+02 1.370e+02 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]] \n",
      "\n",
      "Encoded X Test\n",
      " [[5.500e+01 5.170e+03 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [7.100e+02 9.000e+00 1.726e+03 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.680e+02 1.070e+02 1.200e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.529e+03 2.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [8.520e+02 8.200e+01 3.800e+02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [4.000e+00 2.000e+00 1.740e+02 ... 0.000e+00 0.000e+00 0.000e+00]] \n",
      "\n",
      "Total Words:  10000\n"
     ]
    }
   ],
   "source": [
    "print('Encoded X Train\\n', padded_x_train, '\\n')\n",
    "print('Encoded X Test\\n', padded_x_test, '\\n')\n",
    "print('Total Words: ', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.0</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2895.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3616.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>2308.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8655.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>8178.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130370</th>\n",
       "      <td>285.0</td>\n",
       "      <td>4777.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130371</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130372</th>\n",
       "      <td>126.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130373</th>\n",
       "      <td>584.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>3324.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130374</th>\n",
       "      <td>211.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130375 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1       2       3       4       5       6       7       8    \\\n",
       "0        77.0   107.0    24.0    33.0  1978.0   775.0   104.0   531.0     2.0   \n",
       "1       114.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        49.0  1479.0   216.0   550.0    55.0  2895.0   174.0     3.0    49.0   \n",
       "3        67.0  1838.0     2.0  3616.0   347.0  2308.0  1220.0    24.0  8655.0   \n",
       "4         2.0     7.0    22.0    83.0   769.0  1180.0  8178.0     2.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "130370  285.0  4777.0    88.0     2.0  1011.0     1.0    12.0     1.0    90.0   \n",
       "130371    2.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "130372  126.0   227.0  2836.0   350.0   409.0    29.0    42.0    24.0     2.0   \n",
       "130373  584.0   431.0     4.0   738.0  3324.0   910.0   402.0  2412.0   131.0   \n",
       "130374  211.0   137.0     2.0    11.0   104.0   467.0   241.0   426.0    23.0   \n",
       "\n",
       "           9    ...  126  127  128  129  130  131  132  133  134  135  \n",
       "0        334.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1          0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       2307.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3         65.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4          0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "130370  1262.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "130371     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "130372     6.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "130373   490.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "130374   971.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[130375 rows x 136 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(padded_x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.0</td>\n",
       "      <td>5170.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9224.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>710.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>7400.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32589</th>\n",
       "      <td>681.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5761.0</td>\n",
       "      <td>892.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32590</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32591</th>\n",
       "      <td>1529.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32592</th>\n",
       "      <td>852.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3853.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32593</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>3125.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32594 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1       2       3      4       5       6       7       8    \\\n",
       "0        55.0  5170.0     2.0   130.0   30.0  9224.0   844.0   398.0     0.0   \n",
       "1       710.0     9.0  1726.0     7.0  828.0   523.0     8.0   359.0    13.0   \n",
       "2       268.0   107.0    12.0    79.0   39.0     2.0    75.0   779.0  7400.0   \n",
       "3         2.0   728.0   357.0   115.0  293.0   680.0   605.0   180.0   708.0   \n",
       "4        29.0    42.0   314.0     2.0   88.0    21.0    68.0   314.0  1300.0   \n",
       "...       ...     ...     ...     ...    ...     ...     ...     ...     ...   \n",
       "32589   681.0     4.0   497.0  1168.0    2.0   483.0  1360.0  5761.0   892.0   \n",
       "32590     6.0     4.0     6.0     2.0    0.0     0.0     0.0     0.0     0.0   \n",
       "32591  1529.0     2.0     0.0     0.0    0.0     0.0     0.0     0.0     0.0   \n",
       "32592   852.0    82.0   380.0     2.0   17.0     1.0  3853.0   499.0     2.0   \n",
       "32593     4.0     2.0   174.0   106.0  940.0   934.0  3125.0    14.0  2606.0   \n",
       "\n",
       "         9    ...  126  127  128  129  130  131  132  133  134  135  \n",
       "0        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1        2.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2       19.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      208.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      153.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "32589   61.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32590    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32591    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32592  979.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32593   34.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[32594 rows x 136 columns]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(padded_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(padded_x_train.dtype)\n",
    "print(padded_x_test.dtype)\n",
    "print(y_train_onehot.dtype)\n",
    "print(y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130375, 136)\n",
      "(130375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(padded_x_train.shape)\n",
    "print(y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []  # Use a new list for clarity\n",
    "# for sequence in padded_x_test:\n",
    "#     # Check if all elements in the sequence are integers\n",
    "#     if all(isinstance(x, int) for x in sequence):\n",
    "#         results.append(True)\n",
    "#     else:\n",
    "#         results.append(False)\n",
    "\n",
    "# # You will have a list with True/False for each sequence in padded_x_test\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = False  # Start with False\n",
    "\n",
    "# for sequence in padded_x_test:\n",
    "#     if any(x for x in sequence):  # Check if any element in the sequence is True (non-zero)\n",
    "#         result = True  # If any True found, result becomes True\n",
    "#         break  # No need to check further if we found a True value\n",
    "\n",
    "# print(result)  # This will print True if any True-like element exists, otherwise False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = get_max_length()\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# EMBED_DIM = 32\n",
    "# LSTM_OUT = 64\n",
    "\n",
    "# model = Sequential([\n",
    "#     Input(shape=(max_length,)),\n",
    "#     Embedding(total_words, EMBED_DIM, input_length=max_length),\n",
    "#     LSTM(LSTM_OUT, dropout=0.3, recurrent_dropout=0.3),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# import tensorflow as tf\n",
    "\n",
    "# EMBED_DIM = 32\n",
    "# LSTM_OUT = 64\n",
    "\n",
    "# def custom_activation(x):\n",
    "#     return tf.cast(tf.argmax(x, axis=-1), dtype=tf.float32) - 1\n",
    "\n",
    "# model = Sequential([\n",
    "#     Input(shape=(max_length,)),\n",
    "#     Embedding(total_words, EMBED_DIM, input_length=max_length),\n",
    "#     LSTM(LSTM_OUT, dropout=0.3, recurrent_dropout=0.3),\n",
    "#     Dense(3, activation='linear'),\n",
    "#     tf.keras.layers.Lambda(custom_activation)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     'models/LSTM.keras',\n",
    "#     monitor='accuracy',\n",
    "#     save_best_only=True,\n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.metrics import Precision, Recall\n",
    "\n",
    "# # Hyperparameters\n",
    "# EMBED_DIM = 64  # Increased from 32\n",
    "# LSTM_OUT = 128  # Increased from 64\n",
    "\n",
    "# model = Sequential([\n",
    "#     Input(shape=(max_length,)),\n",
    "#     Embedding(total_words, EMBED_DIM, input_length=max_length),\n",
    "#     Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Bidirectional(LSTM(LSTM_OUT, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "#     Bidirectional(LSTM(LSTM_OUT // 2, dropout=0.3, recurrent_dropout=0.3)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# optimizer = Adam(learning_rate=0.001)  # Increased learning rate\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "# print(model.summary())\n",
    "\n",
    "# # Assume X_train, y_train, X_val, y_val are your training and validation data\n",
    "# # history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "# from keras.metrics import Precision, Recall\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# # Hyperparameters\n",
    "# vocab_size = 5000\n",
    "# embedding_size = 32\n",
    "# epochs = 20\n",
    "# learning_rate = 0.001\n",
    "# decay_rate = learning_rate / epochs\n",
    "# momentum = 0.8\n",
    "\n",
    "# # Define the SGD optimizer\n",
    "# sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_size, input_length=max_length))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Bidirectional(LSTM(32)))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(3, activation='softmax'))  # 3 output classes: negative, neutral, positive\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=sgd,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# # Print model summary\n",
    "# print(model.summary())\n",
    "\n",
    "# # Define learning rate scheduler (optional)\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * 0.1\n",
    "\n",
    "# scheduler = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# # Note: You would train the model like this:\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     epochs=epochs, \n",
    "#                     batch_size=32,\n",
    "#                     callbacks=[scheduler])\n",
    "\n",
    "# # Evaluate the model (after training)\n",
    "# test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Test Precision: {test_precision:.4f}\")\n",
    "# print(f\"Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130375, 136)\n",
      "(130375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(padded_x_train.shape)\n",
    "print(y_train_onehot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │       \u001b[38;5;34m320,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">339,939</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m339,939\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">339,939</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m339,939\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 24ms/step - accuracy: 0.5800 - loss: 0.9075 - precision_2: 0.6436 - recall_2: 0.4277 - val_accuracy: 0.7664 - val_loss: 0.6161 - val_precision_2: 0.7849 - val_recall_2: 0.7418\n",
      "Epoch 2/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 28ms/step - accuracy: 0.7733 - loss: 0.6010 - precision_2: 0.7958 - recall_2: 0.7433 - val_accuracy: 0.7827 - val_loss: 0.5634 - val_precision_2: 0.8016 - val_recall_2: 0.7575\n",
      "Epoch 3/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 30ms/step - accuracy: 0.7881 - loss: 0.5662 - precision_2: 0.8084 - recall_2: 0.7591 - val_accuracy: 0.7888 - val_loss: 0.5522 - val_precision_2: 0.8102 - val_recall_2: 0.7621\n",
      "Epoch 4/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 27ms/step - accuracy: 0.8045 - loss: 0.5264 - precision_2: 0.8254 - recall_2: 0.7772 - val_accuracy: 0.7890 - val_loss: 0.5520 - val_precision_2: 0.8082 - val_recall_2: 0.7681\n",
      "Epoch 5/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 27ms/step - accuracy: 0.8188 - loss: 0.4957 - precision_2: 0.8377 - recall_2: 0.7945 - val_accuracy: 0.7928 - val_loss: 0.5404 - val_precision_2: 0.8112 - val_recall_2: 0.7681\n",
      "Epoch 6/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 27ms/step - accuracy: 0.8276 - loss: 0.4736 - precision_2: 0.8455 - recall_2: 0.8068 - val_accuracy: 0.7909 - val_loss: 0.5705 - val_precision_2: 0.8045 - val_recall_2: 0.7741\n",
      "Epoch 7/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 26ms/step - accuracy: 0.8370 - loss: 0.4481 - precision_2: 0.8543 - recall_2: 0.8177 - val_accuracy: 0.7920 - val_loss: 0.5645 - val_precision_2: 0.8066 - val_recall_2: 0.7763\n",
      "Epoch 8/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 26ms/step - accuracy: 0.8494 - loss: 0.4237 - precision_2: 0.8657 - recall_2: 0.8305 - val_accuracy: 0.7860 - val_loss: 0.5773 - val_precision_2: 0.8004 - val_recall_2: 0.7709\n",
      "Epoch 9/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 28ms/step - accuracy: 0.8575 - loss: 0.4024 - precision_2: 0.8716 - recall_2: 0.8404 - val_accuracy: 0.7865 - val_loss: 0.5755 - val_precision_2: 0.8065 - val_recall_2: 0.7626\n",
      "Epoch 10/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 30ms/step - accuracy: 0.8640 - loss: 0.3839 - precision_2: 0.8798 - recall_2: 0.8478 - val_accuracy: 0.7897 - val_loss: 0.6172 - val_precision_2: 0.7998 - val_recall_2: 0.7773\n",
      "Epoch 11/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 27ms/step - accuracy: 0.8737 - loss: 0.3558 - precision_2: 0.8877 - recall_2: 0.8590 - val_accuracy: 0.7819 - val_loss: 0.6396 - val_precision_2: 0.7937 - val_recall_2: 0.7689\n",
      "Epoch 12/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 28ms/step - accuracy: 0.8787 - loss: 0.3423 - precision_2: 0.8913 - recall_2: 0.8647 - val_accuracy: 0.7846 - val_loss: 0.6521 - val_precision_2: 0.7947 - val_recall_2: 0.7713\n",
      "Epoch 13/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 29ms/step - accuracy: 0.8852 - loss: 0.3259 - precision_2: 0.8980 - recall_2: 0.8725 - val_accuracy: 0.7807 - val_loss: 0.6703 - val_precision_2: 0.7927 - val_recall_2: 0.7656\n",
      "Epoch 14/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 29ms/step - accuracy: 0.8920 - loss: 0.3096 - precision_2: 0.9035 - recall_2: 0.8794 - val_accuracy: 0.7755 - val_loss: 0.6546 - val_precision_2: 0.7904 - val_recall_2: 0.7582\n",
      "Epoch 15/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 30ms/step - accuracy: 0.8985 - loss: 0.2912 - precision_2: 0.9088 - recall_2: 0.8865 - val_accuracy: 0.7759 - val_loss: 0.7463 - val_precision_2: 0.7846 - val_recall_2: 0.7655\n",
      "Epoch 16/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 30ms/step - accuracy: 0.9027 - loss: 0.2810 - precision_2: 0.9130 - recall_2: 0.8911 - val_accuracy: 0.7743 - val_loss: 0.7151 - val_precision_2: 0.7864 - val_recall_2: 0.7590\n",
      "Epoch 17/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 31ms/step - accuracy: 0.9075 - loss: 0.2687 - precision_2: 0.9181 - recall_2: 0.8967 - val_accuracy: 0.7778 - val_loss: 0.7763 - val_precision_2: 0.7855 - val_recall_2: 0.7681\n",
      "Epoch 18/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 30ms/step - accuracy: 0.9090 - loss: 0.2592 - precision_2: 0.9192 - recall_2: 0.8998 - val_accuracy: 0.7675 - val_loss: 0.7454 - val_precision_2: 0.7799 - val_recall_2: 0.7531\n",
      "Epoch 19/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 30ms/step - accuracy: 0.9113 - loss: 0.2554 - precision_2: 0.9210 - recall_2: 0.9018 - val_accuracy: 0.7697 - val_loss: 0.8196 - val_precision_2: 0.7768 - val_recall_2: 0.7607\n",
      "Epoch 20/20\n",
      "\u001b[1m3260/3260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 30ms/step - accuracy: 0.9164 - loss: 0.2404 - precision_2: 0.9254 - recall_2: 0.9079 - val_accuracy: 0.7658 - val_loss: 0.8203 - val_precision_2: 0.7762 - val_recall_2: 0.7544\n",
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.7590 - loss: 0.8461 - precision_2: 0.7676 - recall_2: 0.7489\n",
      "Test Accuracy: 0.7601\n",
      "Test Precision: 0.7696\n",
      "Test Recall: 0.7496\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Input\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "embedding_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "# Define the SGD optimizer\n",
    "sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Input(shape=(max_length,)),\n",
    "    Embedding(vocab_size, embedding_size, input_length=max_length),\n",
    "    Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Note: To train the model, you would use:\n",
    "history = model.fit(padded_x_train, y_train_onehot, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=epochs, \n",
    "                    batch_size=32)\n",
    "\n",
    "# To evaluate:\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(padded_x_test, y_test_onehot, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │       \u001b[38;5;34m320,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">339,939</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m339,939\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">339,939</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m339,939\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Input\n",
    "# from keras.metrics import Precision, Recall\n",
    "# from keras.optimizers import SGD\n",
    "\n",
    "# # Hyperparameters\n",
    "# vocab_size = 10000\n",
    "# embedding_size = 32\n",
    "# epochs = 20\n",
    "# learning_rate = 0.001\n",
    "# decay_rate = learning_rate / epochs\n",
    "# momentum = 0.8\n",
    "\n",
    "# # Define the SGD optimizer\n",
    "# sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential([\n",
    "#     Input(shape=(max_length,)),\n",
    "#     Embedding(vocab_size, embedding_size, input_length=max_length),\n",
    "#     Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Bidirectional(LSTM(32)),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=sgd,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# # Print model summary\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     'models/LSTM.keras',\n",
    "#     monitor='accuracy',\n",
    "#     save_best_only=True,\n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(padded_x_train, y_train_onehot, batch_size = 64, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # predictions = model.predict(padded_x_test)\n",
    "\n",
    "# # Normalize predictions (threshold of 0.5)\n",
    "# # normalized_predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "# # Measure performance metrics\n",
    "# accuracy = accuracy_score(padded_x_test, y_test_onehot)\n",
    "# precision = precision_score(padded_x_test, y_test_onehot)\n",
    "# recall = recall_score(padded_x_test, y_test_onehot)\n",
    "# f1 = f1_score(padded_x_test, y_test_onehot)\n",
    "  \n",
    "# # Print performance metrics\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.43383253, 0.17561305, 0.39055446],\n",
       "       [0.18520503, 0.27510536, 0.5396896 ],\n",
       "       [0.12540895, 0.30616415, 0.5684269 ],\n",
       "       ...,\n",
       "       [0.7101931 , 0.08496027, 0.20484652],\n",
       "       [0.12428288, 0.30786735, 0.56784976],\n",
       "       [0.36034134, 0.20065418, 0.43900448]], dtype=float32)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step\n",
      "Distribution of predictions:\n",
      "(array([0, 1]), array([76601, 21181], dtype=int64))\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.38      0.47     11067\n",
      "           1       0.00      0.00      0.00      7152\n",
      "           2       0.55      0.55      0.55     14375\n",
      "\n",
      "   micro avg       0.57      0.37      0.45     32594\n",
      "   macro avg       0.39      0.31      0.34     32594\n",
      "weighted avg       0.45      0.37      0.40     32594\n",
      " samples avg       0.37      0.37      0.37     32594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_predictions(predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Normalize the model's predictions to binary format.\n",
    "    \n",
    "    Args:\n",
    "    predictions (numpy.ndarray): The model's raw predictions.\n",
    "    threshold (float): The threshold for classification (default is 0.5).\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Normalized binary predictions (0 or 1).\n",
    "    \"\"\"\n",
    "    # Convert to binary predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    return binary_predictions\n",
    "\n",
    "raw_predictions = model.predict(padded_x_test)\n",
    "\n",
    "# Normalize the predictions\n",
    "normalized_predictions = normalize_predictions(raw_predictions)\n",
    "\n",
    "print(\"Distribution of predictions:\")\n",
    "print(np.unique(normalized_predictions, return_counts=True))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_onehot, normalized_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
